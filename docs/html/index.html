<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.15.0"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Bias-in-AI-Generated-Travel-Narratives: Bias-in-AI-Generated-Travel-Narratives</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="clipboard.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript" src="cookie.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Bias-in-AI-Generated-Travel-Narratives
   </div>
   <div id="projectbrief">Documentation for the Bias-in-AI-Generated-Travel-Narratives project.</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.15.0 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search/",'.html');
</script>
<script type="text/javascript">
$(function() { codefold.init(); });
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search',true);
  $(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(function(){initNavTree('index.html','',''); });
</script>
<div id="container">
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">Bias-in-AI-Generated-Travel-Narratives </div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a class="anchor" id="md__r_e_a_d_m_e"></a></p>
<p>This repository contains code, preprocessing pipelines, counterspeech outputs, and analysis used to investigate model-produced counterspeech for travel-related YouTube comments. The README below explains how to reproduce the preliminary results and poster figures for the endterm paper.</p>
<hr  />
<h1 class="doxsection"><a class="anchor" id="autotoc_md2"></a>
Quick overview</h1>
<ul>
<li>Main code areas:<ul>
<li>Data collection: <a class="el" href="youtube__scraper_8py.html">src/data_collection/youtube_scraper.py</a>, <a class="el" href="video__collector_8py.html">src/data_collection/video_collector.py</a></li>
<li>Preprocessing: src/preprocessing/ (filtering, cleaning, jsonljson tools)</li>
<li>Counterspeech model clients &amp; outputs: src/counterspeech/models/ and src/counterspeech/outputs/</li>
<li>Analysis &amp; figures: <a class="el" href="poster_8py.html">src/analysis/poster.py</a></li>
</ul>
</li>
</ul>
<hr  />
<h1 class="doxsection"><a class="anchor" id="autotoc_md4"></a>
Environment &amp; dependencies</h1>
<ul>
<li>Python 3.8+ recommended.</li>
<li>Install dependencies with:</li>
</ul>
<p><span class="tt">&#x2408;ash
python -m pip install -r requirements.txt
</span></p>
<p>It is recommended to use a virtual environment:</p>
<p><span class="tt">&#x2408;ash
python -m venv .venv
@section autotoc_md5 Windows
.\.venv\Scripts\activate
@section autotoc_md6 macOS / Linux
source .venv/bin/activate
python -m pip install -r requirements.txt
</span></p>
<p>Key packages used (listed in equirements.txt): pandas, matplotlib, scikit-learn, bertopic, google-api-python-client, python-dotenv, nltk, pyldavis, and others.</p>
<hr  />
<h2 class="doxsection"><a class="anchor" id="autotoc_md8"></a>
API keys and environment variables</h2>
<ul>
<li>Create a .env file in the project root (or copy .env.example) and add:</li>
</ul>
<p><span class="tt">
YOUTUBE_API_KEY=your_api_key_here
@section autotoc_md9 Optional: keys for Perspective API or other services used by preprocessing
@section autotoc_md10 PERSPECTIVE_API_KEY=your_key_here
</span></p>
<ul>
<li>To obtain a YouTube API key: enable the YouTube Data API v3 in Google Cloud and create an API key under the project's Credentials page.</li>
</ul>
<p>Note: some counterspeech model scripts may call external LLM services or require local model checkpoints; these are optional for reproducing figures because precomputed model outputs are included in the repository.</p>
<hr  />
<h2 class="doxsection"><a class="anchor" id="autotoc_md12"></a>
Included datasets and files</h2>
<ul>
<li>data/raw/ - raw YouTube JSON files (multiple parts are present in the repo).</li>
<li>data/clean/ - cleaned and merged datasets used in analysis (e.g. inal_API_data.json).</li>
<li>src/counterspeech/outputs/ - precomputed counterspeech and Perspective outputs used by <a class="el" href="poster_8py.html">poster.py</a>:<ul>
<li>llama32_scored_dataset_human.json</li>
<li>qwen25_scored_dataset_human.json</li>
<li>llama32_perspective_scores_final.json</li>
<li>qwen25_perspective_scores_final.json</li>
</ul>
</li>
</ul>
<p>If you prefer to re-run the YouTube data yourself, see the section below. If you want to skip heavy model inference, use the provided outputs in src/counterspeech/outputs/.</p>
<hr  />
<h2 class="doxsection"><a class="anchor" id="autotoc_md14"></a>
Reproducing the pipeline (recommended order)</h2>
<p>1) (Optional) Fetch raw YouTube data</p>
<p><span class="tt">&#x2408;ash
@section autotoc_md15 from repository root
python <a class="el" href="youtube__scraper_8py.html">src/data_collection/youtube_scraper.py</a>
@section autotoc_md16 or
python <a class="el" href="video__collector_8py.html">src/data_collection/video_collector.py</a>
</span></p>
<p>These scripts read video IDs from data/video_ids.txt (or data/video_ids.csv), use YOUTUBE_API_KEY from .env, and write raw JSON to data/raw/ or the configured output path.</p>
<p>2) Preprocess and clean</p>
<p>Use the scripts in src/preprocessing/ to filter, clean, and convert jsonl to merged JSON. Example commands:</p>
<p><span class="tt">&#x2408;ash
python <a class="el" href="split__youtube__data_8py.html">src/preprocessing/scripts/split_youtube_data.py</a>
python <a class="el" href="filter_8py.html">src/preprocessing/filter.py</a>
python <a class="el" href="jsonl__to__json_8py.html">src/preprocessing/jsonl_to_json.py</a>
python <a class="el" href="organize_8py.html">src/preprocessing/organize.py</a>
</span></p>
<p>Note: some preprocessing scripts may require API keys if they call external scoring or cleaning APIs. The cleaned files in data/clean/ can be used to skip this step.</p>
<p>3) (Optional / advanced) Generate counterspeech and human scores</p>
<ul>
<li>Running the local model clients (LLaMA / Qwen) may require large model downloads, GPU resources, and additional runtime packages. See <a class="el" href="llama__client_8py.html">src/counterspeech/models/llama_client.py</a> and <a class="el" href="qwen__client_8py.html">qwen_client.py</a>.</li>
<li>To avoid heavy compute, use the precomputed files in src/counterspeech/outputs/ that are included with this repository.</li>
</ul>
<p>4) Generate poster / analysis figures</p>
<p>The script <a class="el" href="poster_8py.html">src/analysis/poster.py</a> reads files from src/counterspeech/outputs/ and writes PNG figures to src/analysis/figures/ (created automatically).</p>
<p>Run the script from the analysis directory so the relative paths match:</p>
<p><span class="tt">&#x2408;ash
cd src/analysis
python <a class="el" href="poster_8py.html">poster.py</a>
</span></p>
<p>This will produce files such as ig1_distribution_human.png, ig_perspective_diff_hist.png, and others under src/analysis/figures/.</p>
<hr  />
<h2 class="doxsection"><a class="anchor" id="autotoc_md18"></a>
Suggested minimal reproducible flow (fast)</h2>
<p>If you only want to reproduce the key figures in the deliverable, do this:</p>
<ol type="1">
<li>Ensure dependencies are installed.</li>
<li>Ensure src/counterspeech/outputs/ contains the four JSON files listed above (these are included in the repo).</li>
<li>Run <a class="el" href="poster_8py.html">poster.py</a> as shown in the previous section.</li>
</ol>
<p>This avoids re-downloading YouTube data and re-running model inference.</p>
<hr  />
<h2 class="doxsection"><a class="anchor" id="autotoc_md20"></a>
Notes, caveats and troubleshooting</h2>
<ul>
<li>Scripts use relative paths; run them from the directory assumed by the script (the README commands show the recommended working directory).</li>
<li>If you plan to re-run model-based counterspeech generation, ensure you have the compute resources and storage for model weights.</li>
<li>If an external API fails (rate limits or missing key), use the included outputs to continue analysis.</li>
</ul>
<hr  />
<h2 class="doxsection"><a class="anchor" id="autotoc_md22"></a>
Contact</h2>
<p>If you have issues reproducing the results or want help adapting the pipeline, open an issue in the repository or contact the project owner.</p>
<hr  />
<p><em>Prepared for the endterm paper reproducibility deliverable.</em> </p>
</div></div><!-- PageDoc -->
<a href="doxygen_crawl.html"></a>
</div><!-- contents -->
</div><!-- doc-content -->
</div><!-- container -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.15.0 </li>
  </ul>
</div>
</body>
</html>
